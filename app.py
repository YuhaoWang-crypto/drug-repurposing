import streamlit as st
import pandas as pd
import numpy as np
import requests
import re
import os
import shutil
import time
import json
import gzip
import tarfile
from pathlib import Path
from scipy import stats
from statsmodels.stats.multitest import multipletests
from bs4 import BeautifulSoup
import mygene
from urllib.parse import quote
import matplotlib.pyplot as plt
import seaborn as sns

# ==========================================
# 0. é…ç½®ä¸åˆå§‹åŒ–
# ==========================================
st.set_page_config(page_title="GEO Drug Repurposing Pipeline", layout="wide", page_icon="ğŸ’Š")

# å®šä¹‰å·¥ä½œç›®å½•
WORK_DIR = Path("workspace")
RAW_DIR = WORK_DIR / "raw"
PROC_DIR = WORK_DIR / "proc"
RAW_DIR.mkdir(parents=True, exist_ok=True)
PROC_DIR.mkdir(parents=True, exist_ok=True)

# Session State åˆå§‹åŒ–
if "geo_hits" not in st.session_state:
    st.session_state["geo_hits"] = pd.DataFrame()
if "selected_gses" not in st.session_state:
    st.session_state["selected_gses"] = []
if "final_drug_rank" not in st.session_state:
    st.session_state["final_drug_rank"] = pd.DataFrame()

# ==========================================
# 1. æ ¸å¿ƒå·¥å…·å‡½æ•°
# ==========================================

@st.cache_resource
def get_mygene_info():
    return mygene.MyGeneInfo()

def clean_gene_list(genes):
    out = []
    seen = set()
    for g in genes:
        if not isinstance(g, str): continue
        # å»é™¤Ensemblç‰ˆæœ¬å· æˆ– /// åˆ†éš”ç¬¦
        g = g.split(".")[0].split("//")[0].strip().upper()
        if g and g not in seen:
            seen.add(g)
            out.append(g)
    return out

# --- GEO ä¸‹è½½ä¸è§£æ ---

def geo_search(query, retmax=30):
    """æœç´¢ GEO æ•°æ®é›†"""
    base = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    try:
        search_url = f"{base}/esearch.fcgi?db=gds&term={quote(query)}&retmax={retmax}&retmode=json"
        r = requests.get(search_url, timeout=10).json()
        ids = r.get("esearchresult", {}).get("idlist", [])
        if not ids: return pd.DataFrame()
        
        summary_url = f"{base}/esummary.fcgi?db=gds&id={','.join(ids)}&retmode=json"
        r = requests.get(summary_url, timeout=10).json()
        result = r.get("result", {})
        
        rows = []
        for uid in ids:
            if uid not in result: continue
            item = result[uid]
            acc = item.get("accession", "")
            if not acc.startswith("GSE"): continue
            rows.append({
                "Accession": acc,
                "Title": item.get("title", ""),
                "Summary": item.get("summary", "")[:200] + "...",
                "Taxon": item.get("taxon", ""),
                "Samples": item.get("n_samples", 0),
                "Date": item.get("pdat", "")
            })
        return pd.DataFrame(rows)
    except Exception as e:
        st.error(f"Search failed: {e}")
        return pd.DataFrame()

def download_file(url, path):
    """ä¸‹è½½æ–‡ä»¶ï¼Œå¦‚æœå­˜åœ¨åˆ™è·³è¿‡"""
    path = Path(path)
    if path.exists() and path.stat().st_size > 0: return
    try:
        r = requests.get(url, stream=True, timeout=60)
        r.raise_for_status()
        with open(path, 'wb') as f:
            for chunk in r.iter_content(chunk_size=1024*1024):
                if chunk: f.write(chunk)
    except Exception as e:
        if path.exists(): path.unlink() # åˆ é™¤æŸåæ–‡ä»¶
        raise e

def get_geo_urls(gse):
    """ç”Ÿæˆä¸‹è½½é“¾æ¥"""
    gse = gse.strip().upper()
    # æå–æ•°å­—éƒ¨åˆ†ç”¨äºæ„å»ºç›®å½•ï¼Œä¾‹å¦‚ GSE12345 -> GSE12nnn
    num = re.findall(r'\d+', gse)
    if not num: return "", ""
    series_id = int(num[0])
    prefix = f"GSE{series_id // 1000}nnn"
    
    soft_url = f"https://ftp.ncbi.nlm.nih.gov/geo/series/{prefix}/{gse}/soft/{gse}_family.soft.gz"
    matrix_url = f"https://ftp.ncbi.nlm.nih.gov/geo/series/{prefix}/{gse}/matrix/{gse}_series_matrix.txt.gz"
    return soft_url, matrix_url

def parse_soft_robust(soft_path, case_terms, ctrl_terms):
    """
    å¢å¼ºç‰ˆ Soft è§£æï¼šè¯»å– Title, Source, Characteristics, Description
    è¿”å›: (conditions_series, debug_info_dict)
    """
    meta = {}
    current_gsm = None
    
    # é€è¡Œè¯»å– Soft æ–‡ä»¶
    with gzip.open(soft_path, 'rt', encoding='utf-8', errors='ignore') as f:
        for line in f:
            line = line.strip()
            if line.startswith("^SAMPLE ="):
                current_gsm = line.split("=")[1].strip()
                meta[current_gsm] = [] # ä½¿ç”¨åˆ—è¡¨å­˜å‚¨è¯¥æ ·æœ¬çš„æ‰€æœ‰æè¿°æ–‡æœ¬
            elif current_gsm:
                # æŠ“å–æ‰€æœ‰å¯èƒ½åŒ…å«åˆ†ç»„ä¿¡æ¯çš„å­—æ®µ
                if line.startswith(("!Sample_title", "!Sample_source_name", "!Sample_characteristics", "!Sample_description")):
                    try:
                        content = line.split("=", 1)[1].strip().lower()
                        meta[current_gsm].append(content)
                    except:
                        pass

    conditions = {}
    debug_info = {} # ç”¨äºåœ¨ç•Œé¢ä¸Šå±•ç¤ºï¼Œå¸®åŠ©ç”¨æˆ·Debug

    for gsm, texts in meta.items():
        full_text = " | ".join(texts) # åˆå¹¶æ‰€æœ‰ä¿¡æ¯
        debug_info[gsm] = full_text   # ä¿å­˜ç»™ç”¨æˆ·çœ‹
        
        # åŒ¹é…é€»è¾‘
        hit_case = any(t in full_text for t in case_terms)
        hit_ctrl = any(t in full_text for t in ctrl_terms)
        
        if hit_case and not hit_ctrl:
            conditions[gsm] = "case"
        elif hit_ctrl and not hit_case:
            conditions[gsm] = "control"
        elif hit_case and hit_ctrl:
            # å†²çªå¤„ç†ï¼šé€šå¸¸ Case çš„æè¿°ï¼ˆå¦‚ specific mutationï¼‰æ¯” Control æ›´ç‰¹å¼‚
            # å¦‚æœåŒ…å« disease/mutationï¼Œç”±äº control æ ·æœ¬ä¹Ÿå¯èƒ½æåˆ° disease (e.g. "control for disease X")
            # è¿™é‡Œä¿å®ˆèµ·è§è®¾ä¸º ambiguousï¼Œæˆ–è€…ä½ å¯ä»¥åå‘ Case
            conditions[gsm] = "ambiguous"
        else:
            conditions[gsm] = "unknown"
            
    return pd.Series(conditions), debug_info

# --- å·®å¼‚åˆ†æä¸»æµç¨‹ ---

def run_analysis_pipeline(gse, case_terms, ctrl_terms):
    """ä¸‹è½½ -> è§£æ -> å·®å¼‚åˆ†æ"""
    gse_dir = RAW_DIR / gse
    gse_dir.mkdir(exist_ok=True)
    
    soft_url, matrix_url = get_geo_urls(gse)
    soft_path = gse_dir / f"{gse}_family.soft.gz"
    matrix_path = gse_dir / f"{gse}_series_matrix.txt.gz"
    
    # 1. ä¸‹è½½
    try:
        download_file(soft_url, soft_path)
        download_file(matrix_url, matrix_path)
    except Exception as e:
        return None, f"Download Error: {str(e)}", {}

    # 2. åˆ†ç»„è§£æ (å…³é”®æ­¥éª¤)
    conditions, debug_info = parse_soft_robust(soft_path, case_terms, ctrl_terms)
    
    case_samps = conditions[conditions == "case"].index.tolist()
    ctrl_samps = conditions[conditions == "control"].index.tolist()
    
    # å¦‚æœåˆ†ç»„å¤±è´¥ï¼Œç›´æ¥è¿”å›è°ƒè¯•ä¿¡æ¯
    if len(case_samps) == 0 or len(ctrl_samps) == 0:
        msg = f"Insufficient Samples: Case={len(case_samps)}, Ctrl={len(ctrl_samps)}"
        return None, msg, debug_info
    
    # 3. è¯»å–çŸ©é˜µ
    try:
        # matrixæ–‡ä»¶é€šå¸¸ headeræ¯”è¾ƒä¹±ï¼Œskiprows=... éœ€è¦è‡ªåŠ¨åˆ¤æ–­ï¼Œè¿™é‡Œå‡è®¾æ ‡å‡†æ ¼å¼ !series_matrix_table_begin ä¸‹ä¸€è¡Œæ˜¯header
        # ç®€å•å¤„ç†ï¼šç›´æ¥ read_csv, comment='!'
        df = pd.read_csv(matrix_path, sep="\t", comment="!", index_col=0, on_bad_lines='skip')
        df = df.dropna(how='all')
        df = df.fillna(0)
        
        # ç®€å•çš„æ•°æ®å˜æ¢åˆ¤æ–­
        if df.max().max() > 50:
            df = np.log2(df + 1)
    except Exception as e:
        return None, f"Matrix Parse Error: {str(e)}", debug_info
    
    # å¯¹é½
    # çŸ©é˜µåˆ—åå¯èƒ½æ˜¯ GSMxxxxx ä¹Ÿå¯èƒ½æ˜¯ "GSMxxxxx_sample_name"ï¼Œåšæ¨¡ç³ŠåŒ¹é…
    valid_cols = []
    col_map = {} # Matrix Col -> GSM
    
    for col in df.columns:
        # å°è¯•æå– col ä¸­çš„ GSM
        m = re.search(r'(GSM\d+)', col)
        if m:
            gsm = m.group(1)
            if gsm in conditions.index:
                valid_cols.append(col)
                col_map[col] = gsm
    
    if len(valid_cols) < 2:
        return None, f"Column Mismatch: Matrix columns do not match SOFT GSM IDs. Found: {list(df.columns[:5])}", debug_info
    
    df = df[valid_cols]
    
    # æ˜ å°„å› condition
    case_cols = [c for c in valid_cols if conditions.get(col_map[c]) == "case"]
    ctrl_cols = [c for c in valid_cols if conditions.get(col_map[c]) == "control"]
    
    if len(case_cols) == 0 or len(ctrl_cols) == 0:
         return None, f"Aligned Samples Missing: Case={len(case_cols)}, Ctrl={len(ctrl_cols)}", debug_info

    # 4. å·®å¼‚åˆ†æ (T-test æˆ– Mean Diff)
    results = []
    use_ttest = len(case_cols) >= 2 and len(ctrl_cols) >= 2
    
    # ä¸ºé€Ÿåº¦è€ƒè™‘ï¼Œå¦‚æœä¸ä½¿ç”¨ pydeseq2ï¼Œè¿™é‡Œç”¨ numpy å‘é‡åŒ–è®¡ç®—ä¼šæ›´å¿«
    # è¿™é‡Œç”¨ iterrows è™½ç„¶æ…¢ç‚¹ä½†ç¨³å¥
    for gene, row in df.iterrows():
        case_vals = row[case_cols].values
        ctrl_vals = row[ctrl_cols].values
        
        diff = np.mean(case_vals) - np.mean(ctrl_vals)
        p = 1.0
        
        if use_ttest:
            # å¿½ç•¥å…¨ä¸º0æˆ–æ–¹å·®æå°çš„æƒ…å†µ
            if np.std(case_vals) < 1e-6 and np.std(ctrl_vals) < 1e-6:
                p = 1.0
            else:
                try:
                    _, p = stats.ttest_ind(case_vals, ctrl_vals, equal_var=False)
                except:
                    p = 1.0
        
        results.append({"gene": gene, "log2fc": diff, "pval": p})
        
    res_df = pd.DataFrame(results)
    if res_df.empty: return None, "No valid DE results", debug_info
    
    # FDR æ ¡æ­£
    res_df["pval"] = res_df["pval"].fillna(1.0)
    res_df["padj"] = multipletests(res_df["pval"], method="fdr_bh")[1]
    res_df = res_df.sort_values("log2fc", key=abs, ascending=False) # æŒ‰ LogFC ç»å¯¹å€¼æ’åº
    
    # æå–åŸºå› å (å»é™¤ /// æˆ– ID)
    res_df["gene_symbol"] = res_df["gene"].apply(lambda x: str(x).split("//")[0].split(".")[0].strip().upper())
    
    return res_df, f"Success: Case={len(case_cols)}, Ctrl={len(ctrl_cols)}", debug_info

# --- Connectivity API ---

def run_l1000fwd(up_genes, dn_genes):
    url = "https://maayanlab.cloud/l1000fwd/sig_search"
    # L1000FWD å¯¹åŸºå› æ•°é‡æœ‰é™åˆ¶ï¼Œä¸”å¿…é¡»æ˜¯å¤§å†™ Symbol
    payload = {"up_genes": up_genes[:150], "down_genes": dn_genes[:150]}
    try:
        r = requests.post(url, json=payload, timeout=30)
        res_id = r.json().get("result_id")
        if not res_id: return pd.DataFrame()
        
        time.sleep(1)
        r2 = requests.get(f"https://maayanlab.cloud/l1000fwd/result/topn/{res_id}", timeout=30)
        data = r2.json()
        
        rows = []
        # æˆ‘ä»¬ä¸»è¦å…³æ³¨ 'opposite' (åè½¬ gene signature çš„è¯ç‰©)
        if "opposite" in data:
            for item in data["opposite"]:
                rows.append({
                    "drug": item.get("pert_id"), # L1000FWD è¿”å›çš„æ˜¯ ID æˆ– Name
                    "score": item.get("score"),
                    "source": "L1000FWD",
                    "direction": "opposite"
                })
        return pd.DataFrame(rows)
    except Exception as e:
        print(f"L1000FWD Error: {e}")
        return pd.DataFrame()

def run_enrichr(genes, library="LINCS_L1000_Chem_Pert_down"):
    base = "https://maayanlab.cloud/Enrichr"
    try:
        # 1. Add List
        r = requests.post(f"{base}/addList", files={
            'list': (None, '\n'.join(genes[:300])),
            'description': (None, 'Streamlit_Pipeline')
        }, timeout=30)
        user_list_id = r.json().get("userListId")
        if not user_list_id: return pd.DataFrame()
        
        # 2. Enrich
        r2 = requests.get(f"{base}/enrich?userListId={user_list_id}&backgroundType={library}", timeout=30)
        data = r2.json()
        if library not in data: return pd.DataFrame()
        
        rows = []
        for item in data[library]:
            # Enrichr ç»“æœæ ¼å¼: [Rank, Term, P-value, Z-score, Combined Score, ...]
            # Term é€šå¸¸æ˜¯ "DrugName_CellLine_..."
            term = item[1]
            drug_name = term.split("_")[0].split(" ")[0] # ç®€å•æ¸…æ´—
            
            rows.append({
                "drug": drug_name,
                "score": item[4], # Combined Score
                "pval": item[2],
                "source": "Enrichr"
            })
        return pd.DataFrame(rows)
    except Exception as e:
        print(f"Enrichr Error: {e}")
        return pd.DataFrame()

# ==========================================
# 2. Streamlit ç•Œé¢é€»è¾‘
# ==========================================

# --- Sidebar ---
with st.sidebar:
    st.header("âš™ï¸ å‚æ•°è®¾ç½®")
    taxon_filter = st.selectbox("ç‰©ç§è¿‡æ»¤", ["Homo sapiens", "Mus musculus", "All"], index=0)
    
    st.divider()
    st.markdown("### ğŸ·ï¸ åˆ†ç»„å…³é”®è¯ (å…³é”®)")
    st.info("å¦‚æœæ‰¾ä¸åˆ°æ ·æœ¬ï¼Œè¯·åœ¨è¿™é‡Œæ·»åŠ æ ·æœ¬æè¿°ä¸­å‡ºç°çš„è¯ã€‚")
    
    # é’ˆå¯¹ä½ ä¹‹å‰ CLCN / Cystic Fibrosis ä¼˜åŒ–çš„é»˜è®¤å…³é”®è¯
    default_case = "mutation, mutant, variant, patient, knockout, knockdown, disease, clcn, cf, cystic fibrosis, tumor, cancer, treated, stimulation, infected"
    default_ctrl = "control, wt, wild type, wild-type, healthy, normal, vehicle, pbs, dmso, mock, baseline, untreated, placebo, non-targeting"
    
    case_input = st.text_area("å®éªŒç»„ (Case) å…³é”®è¯", default_case, height=100)
    ctrl_input = st.text_area("å¯¹ç…§ç»„ (Control) å…³é”®è¯", default_ctrl, height=100)
    
    case_terms = [x.strip().lower() for x in case_input.split(",") if x.strip()]
    ctrl_terms = [x.strip().lower() for x in ctrl_input.split(",") if x.strip()]
    
    st.divider()
    top_n_genes = st.number_input("Signature åŸºå› æ•°é‡ (Top N)", 50, 500, 150)
    st.caption("æå–å¤šå°‘ä¸ªå·®å¼‚åŸºå› ç”¨äºè¯ç‰©é¢„æµ‹")

# --- Main Tabs ---
tab1, tab2, tab3 = st.tabs(["1ï¸âƒ£ æœç´¢ & é€‰æ‹©", "2ï¸âƒ£ è¿è¡Œæ‰¹å¤„ç†", "3ï¸âƒ£ ç»“æœçœ‹æ¿"])

# --- Tab 1: Search ---
with tab1:
    st.subheader("ğŸ” æœç´¢ GEO æ•°æ®é›†")
    col1, col2 = st.columns([3, 1])
    with col1:
        query_text = st.text_input("è¾“å…¥æŸ¥è¯¢", value='(CLCN2 OR "chloride channel 2") AND (mutation OR knockout) AND "RNA-seq"')
    with col2:
        search_btn = st.button("å¼€å§‹æœç´¢", use_container_width=True)
    
    if search_btn and query_text:
        with st.spinner("æ­£åœ¨è¿æ¥ NCBI..."):
            df_hits = geo_search(query_text)
            if not df_hits.empty:
                if taxon_filter != "All":
                    df_hits = df_hits[df_hits["Taxon"] == taxon_filter]
                st.session_state["geo_hits"] = df_hits
            else:
                st.warning("æœªæ‰¾åˆ°ç»“æœï¼Œè¯·æ”¾å®½å…³é”®è¯æˆ–æ£€æŸ¥ç½‘ç»œã€‚")
    
    if not st.session_state["geo_hits"].empty:
        st.write(f"æ‰¾åˆ° {len(st.session_state['geo_hits'])} ä¸ªæ•°æ®é›† (è¯·å‹¾é€‰è¦åˆ†æçš„):")
        
        # ä½¿ç”¨ DataEditor é€‰æ‹©
        hits_display = st.session_state["geo_hits"].copy()
        hits_display.insert(0, "Select", False)
        
        edited_df = st.data_editor(
            hits_display,
            column_config={"Select": st.column_config.CheckboxColumn(required=True)},
            disabled=["Accession", "Title", "Summary", "Taxon", "Samples", "Date"],
            use_container_width=True,
            hide_index=True
        )
        
        selected = edited_df[edited_df["Select"]]["Accession"].tolist()
        st.session_state["selected_gses"] = selected
        
        if selected:
            st.success(f"å·²é€‰æ‹© {len(selected)} ä¸ªæ•°æ®é›†: {', '.join(selected)}")
            st.info("ğŸ‘‰ è¯·å‰å¾€ '2ï¸âƒ£ è¿è¡Œæ‰¹å¤„ç†' æ ‡ç­¾é¡µå¼€å§‹åˆ†æ")
    else:
        st.write("æš‚æ— æ•°æ®ã€‚")

# --- Tab 2: Run ---
with tab2:
    st.subheader("âš¡ è‡ªåŠ¨åŒ–åˆ†æ Pipeline")
    
    if not st.session_state["selected_gses"]:
        st.warning("âš ï¸ è¯·å…ˆåœ¨ç¬¬ 1 æ­¥é€‰æ‹©è‡³å°‘ä¸€ä¸ªæ•°æ®é›†ã€‚")
    else:
        st.markdown(f"**å¾…åˆ†æåˆ—è¡¨**: {', '.join(st.session_state['selected_gses'])}")
        
        if st.button("ğŸš€ å¯åŠ¨åˆ†æ (Start Pipeline)", type="primary"):
            results_bucket = []
            log_area = st.container()
            progress_bar = st.progress(0)
            
            total = len(st.session_state["selected_gses"])
            
            for i, gse in enumerate(st.session_state["selected_gses"]):
                with log_area:
                    st.write(f"--- å¤„ç†ä¸­: **{gse}** ({i+1}/{total}) ---")
                    
                    # 1. å·®å¼‚åˆ†æ
                    df_de, msg, debug_info = run_analysis_pipeline(gse, case_terms, ctrl_terms)
                    
                    if df_de is None:
                        st.error(f"âŒ {gse} å¤±è´¥: {msg}")
                        # === DEBUG å…³é”®ç‚¹ ===
                        with st.expander(f"ğŸ•µï¸â€â™‚ï¸ è°ƒè¯•: {gse} çš„æ ·æœ¬å…ƒæ•°æ® (ä¸ºä»€ä¹ˆæ²¡åŒ¹é…åˆ°?)"):
                            st.caption("ç³»ç»Ÿè¯»å–åˆ°çš„æ ·æœ¬æè¿°å¦‚ä¸‹ã€‚è¯·æ£€æŸ¥è¿™äº›æ–‡æœ¬ï¼Œæ‰¾å‡ºä»£è¡¨ Case/Control çš„ç‰¹å®šè¯æ±‡ï¼Œå¹¶æ·»åŠ åˆ°å·¦ä¾§è®¾ç½®æ ã€‚")
                            # åªæ˜¾ç¤ºå‰ 15 ä¸ªæ ·æœ¬ï¼Œé¿å…å¤ªé•¿
                            preview_keys = list(debug_info.keys())[:15]
                            st.json({k: debug_info[k] for k in preview_keys})
                        continue
                    
                    st.success(f"âœ… {gse} å·®å¼‚åˆ†æå®Œæˆ: {msg}")
                    
                    # 2. æå– Signature
                    # åªæœ‰ LogFC å¤§çš„æ‰ç®— Upï¼Œå°çš„æ‰ç®— Down
                    up_genes = df_de[df_de["log2fc"] > 0].head(top_n_genes)["gene_symbol"].tolist()
                    dn_genes = df_de[df_de["log2fc"] < 0].tail(top_n_genes)["gene_symbol"].tolist()
                    
                    up_genes = clean_gene_list(up_genes)
                    dn_genes = clean_gene_list(dn_genes)
                    
                    if len(up_genes) < 10 or len(dn_genes) < 10:
                        st.warning(f"âš ï¸ {gse}: å·®å¼‚åŸºå› è¿‡å°‘ (Up={len(up_genes)}, Down={len(dn_genes)})ï¼Œè·³è¿‡è¯ç‰©é¢„æµ‹ã€‚")
                        continue
                        
                    # 3. è¯ç‰©é¢„æµ‹ API
                    st.text(f"æ­£åœ¨æŸ¥è¯¢ L1000FWD å’Œ Enrichr...")
                    
                    # L1000FWD (æ‰¾åè½¬)
                    df_l1000 = run_l1000fwd(up_genes, dn_genes)
                    
                    # Enrichr (UP genes vs Drug Down) -> Reversal
                    df_enrichr = run_enrichr(up_genes, library="LINCS_L1000_Chem_Pert_down")
                    # Enrichr (Down genes vs Drug Up) -> Reversal (Optional, add if needed)
                    
                    # åˆå¹¶
                    parts = []
                    if not df_l1000.empty: parts.append(df_l1000)
                    if not df_enrichr.empty: parts.append(df_enrichr)
                    
                    if parts:
                        combined = pd.concat(parts)
                        combined["gse"] = gse
                        results_bucket.append(combined)
                        with st.expander(f"ğŸ’Š {gse} é¢„æµ‹åˆ°çš„ Top è¯ç‰©"):
                            st.dataframe(combined.head(5))
                    else:
                        st.warning(f"{gse}: API æœªè¿”å›æœ‰æ•ˆè¯ç‰©ç»“æœã€‚")
                
                progress_bar.progress((i + 1) / total)
            
            st.success("ğŸ‰ æ‰€æœ‰ä»»åŠ¡å¤„ç†å®Œæ¯•ï¼è¯·æŸ¥çœ‹ '3ï¸âƒ£ ç»“æœçœ‹æ¿'")
            if results_bucket:
                st.session_state["final_drug_rank"] = pd.concat(results_bucket)
            else:
                st.session_state["final_drug_rank"] = pd.DataFrame()

# --- Tab 3: Results ---
with tab3:
    st.subheader("ğŸ’Š è¯ç‰©æ±‡æ€»ä¸æ’åº")
    
    res = st.session_state.get("final_drug_rank", pd.DataFrame())
    
    if not res.empty:
        # æ¸…æ´—è¯å (è½¬å°å†™ï¼Œå»é™¤éæ³•å­—ç¬¦)
        res["drug_clean"] = res["drug"].astype(str).str.lower().str.strip()
        # å»æ‰ BRD-xxxx è¿™ç§å†…éƒ¨IDï¼Œå¦‚æœå¤ªçŸ­çš„é€šå¸¸ä¸æ˜¯å¥½è¯å
        res = res[res["drug_clean"].str.len() > 2]
        
        # èšåˆç»Ÿè®¡
        agg_df = res.groupby("drug_clean").agg(
            Frequency=('gse', 'nunique'),         # åœ¨å¤šå°‘ä¸ª GSE ä¸­å‡ºç°
            Total_Score=('score', 'sum'),         # æ€»åˆ† (ä»…ä¾›å‚è€ƒï¼Œä¸åŒæºåˆ†æ•°ä¸å¯ç›´æ¥åŠ )
            Sources=('source', lambda x: ", ".join(sorted(set(x)))),
            Support_GSEs=('gse', lambda x: ", ".join(sorted(set(x))))
        ).reset_index()
        
        # æ’åºï¼šä¼˜å…ˆæŒ‰å‡ºç°é¢‘ç‡ï¼Œå…¶æ¬¡æŒ‰æ€»åˆ†
        agg_df = agg_df.sort_values(["Frequency", "Total_Score"], ascending=[False, False])
        agg_df.columns = ["Drug Name", "GSE Count", "Sum Score", "Sources", "GSE IDs"]
        
        # å±•ç¤º Top ç»“æœ
        col_view, col_stat = st.columns([3, 1])
        
        with col_view:
            st.markdown("### ğŸ† Top å€™é€‰è¯ç‰©åˆ—è¡¨")
            st.dataframe(
                agg_df.style.background_gradient(subset=["GSE Count"], cmap="Greens"),
                use_container_width=True,
                height=600
            )
        
        with col_stat:
            st.markdown("### ğŸ“Š ç»Ÿè®¡")
            st.metric("æ€»è¯ç‰©æ•°", len(agg_df))
            st.metric("é«˜ç½®ä¿¡åº¦ (>1 GSE)", len(agg_df[agg_df["GSE Count"] > 1]))
            
            st.download_button(
                "ğŸ“¥ ä¸‹è½½å®Œæ•´ CSV",
                data=agg_df.to_csv(index=False).encode("utf-8"),
                file_name="drug_repurposing_final_rank.csv",
                mime="text/csv",
                type="primary"
            )
            
            st.markdown("---")
            st.info("æç¤º: GSE Count è¶Šé«˜ï¼Œä»£è¡¨è¯¥è¯ç‰©åœ¨å¤šä¸ªç‹¬ç«‹æ•°æ®é›†ä¸­å‡æ˜¾ç¤ºå‡ºå¯¹ç–¾ç—…ç‰¹å¾çš„åè½¬ä½œç”¨ï¼Œå¯é æ€§è¶Šé«˜ã€‚")
            
    else:
        st.info("æš‚æ— ç»“æœã€‚è¯·å…ˆè¿è¡Œ Pipelineï¼Œå¹¶ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªæ•°æ®é›†æˆåŠŸè·‘é€šã€‚")
