import streamlit as st
import pandas as pd
import numpy as np
import requests
import re
import os
import shutil
import time
import json
import gzip
import tarfile
from pathlib import Path
from scipy import stats
from statsmodels.stats.multitest import multipletests
from bs4 import BeautifulSoup
import mygene
from urllib.parse import quote
import matplotlib.pyplot as plt
import seaborn as sns

# ==========================================
# 0. é…ç½®ä¸åˆå§‹åŒ–
# ==========================================
st.set_page_config(page_title="GEO Drug Repurposing Pipeline", layout="wide")

# å®šä¹‰å·¥ä½œç›®å½• (ä½¿ç”¨ Streamlit çš„ä¸´æ—¶ç›®å½•æˆ–æœ¬åœ°ç›®å½•)
WORK_DIR = Path("workspace")
RAW_DIR = WORK_DIR / "raw"
PROC_DIR = WORK_DIR / "proc"
RAW_DIR.mkdir(parents=True, exist_ok=True)
PROC_DIR.mkdir(parents=True, exist_ok=True)

# ç¼“å­˜é…ç½®
st.session_state.setdefault("geo_hits", pd.DataFrame())
st.session_state.setdefault("analysis_results", [])
st.session_state.setdefault("final_drug_rank", pd.DataFrame())

# ==========================================
# 1. å·¥å…·å‡½æ•° (ä»åŸè„šæœ¬ç²¾ç®€ç§»æ¤)
# ==========================================

@st.cache_resource
def get_mygene_info():
    return mygene.MyGeneInfo()

def clean_gene_list(genes):
    out = []
    seen = set()
    for g in genes:
        if not isinstance(g, str): continue
        g = g.split(".")[0].strip().upper() # å»é™¤Ensemblç‰ˆæœ¬å·
        if g and g not in seen:
            seen.add(g)
            out.append(g)
    return out

def map_to_symbols(genes, species="human"):
    mg = get_mygene_info()
    genes = clean_gene_list(genes)
    if not genes: return []
    
    # ç®€å•çš„æ‰¹é‡æŸ¥è¯¢
    res = mg.querymany(genes, scopes=["symbol", "ensembl.gene", "entrezgene"], 
                       fields="symbol", species=species, verbose=False, returnall=False)
    
    symbols = []
    for r in res:
        if "symbol" in r:
            symbols.append(r["symbol"].upper())
    return list(set(symbols))

# --- GEO ä¸‹è½½ä¸è§£æ ---
def geo_search(query, retmax=20):
    base = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    # Search
    search_url = f"{base}/esearch.fcgi?db=gds&term={quote(query)}&retmax={retmax}&retmode=json"
    r = requests.get(search_url).json()
    ids = r.get("esearchresult", {}).get("idlist", [])
    if not ids: return pd.DataFrame()
    
    # Summary
    summary_url = f"{base}/esummary.fcgi?db=gds&id={','.join(ids)}&retmode=json"
    r = requests.get(summary_url).json()
    result = r.get("result", {})
    
    rows = []
    for uid in ids:
        if uid not in result: continue
        item = result[uid]
        if not item.get("accession", "").startswith("GSE"): continue
        rows.append({
            "Accession": item.get("accession"),
            "Title": item.get("title"),
            "Summary": item.get("summary", "")[:200] + "...",
            "Taxon": item.get("taxon", ""),
            "Samples": item.get("n_samples"),
            "Date": item.get("pdat")
        })
    return pd.DataFrame(rows)

def download_file(url, path):
    path = Path(path)
    if path.exists() and path.stat().st_size > 0: return
    r = requests.get(url, stream=True)
    with open(path, 'wb') as f:
        for chunk in r.iter_content(chunk_size=1024*1024):
            if chunk: f.write(chunk)

def get_geo_urls(gse):
    # ç®€åŒ–çš„ URL ç”Ÿæˆ
    gse = gse.upper()
    prefix = gse[:-3] + "nnn"
    soft_url = f"https://ftp.ncbi.nlm.nih.gov/geo/series/{prefix}/{gse}/soft/{gse}_family.soft.gz"
    matrix_url = f"https://ftp.ncbi.nlm.nih.gov/geo/series/{prefix}/{gse}/matrix/{gse}_series_matrix.txt.gz"
    return soft_url, matrix_url

def parse_soft_conditions(soft_path, case_terms, ctrl_terms):
    # æç®€ç‰ˆ Soft è§£æä¸åˆ†ç»„
    meta = {}
    current_gsm = None
    
    with gzip.open(soft_path, 'rt', errors='ignore') as f:
        for line in f:
            line = line.strip()
            if line.startswith("^SAMPLE ="):
                current_gsm = line.split("=")[1].strip()
                meta[current_gsm] = ""
            elif current_gsm and (line.startswith("!Sample_title") or line.startswith("!Sample_source_name")):
                meta[current_gsm] += " " + line.split("=")[1].strip().lower()
                
    conditions = {}
    for gsm, text in meta.items():
        is_case = any(t in text for t in case_terms)
        is_ctrl = any(t in text for t in ctrl_terms)
        
        if is_case and not is_ctrl: conditions[gsm] = "case"
        elif is_ctrl and not is_case: conditions[gsm] = "control"
        else: conditions[gsm] = "ambiguous"
        
    return pd.Series(conditions)

# --- å·®å¼‚åˆ†æ ---
def run_ttest_pipeline(gse, case_terms, ctrl_terms):
    gse_dir = RAW_DIR / gse
    gse_dir.mkdir(exist_ok=True)
    
    soft_url, matrix_url = get_geo_urls(gse)
    soft_path = gse_dir / f"{gse}_family.soft.gz"
    matrix_path = gse_dir / f"{gse}_series_matrix.txt.gz"
    
    # 1. Download
    download_file(soft_url, soft_path)
    download_file(matrix_url, matrix_path)
    
    # 2. Parse Conditions
    conditions = parse_soft_conditions(soft_path, case_terms, ctrl_terms)
    case_samples = conditions[conditions == "case"].index.tolist()
    ctrl_samples = conditions[conditions == "control"].index.tolist()
    
    if len(case_samples) < 2 or len(ctrl_samples) < 2:
        return None, f"Insufficient samples: Case={len(case_samples)}, Ctrl={len(ctrl_samples)}"
    
    # 3. Load Matrix
    try:
        df = pd.read_csv(matrix_path, sep="\t", comment="!", index_col=0)
        # ç®€å•æ¸…æ´—ï¼šå»é™¤ç©ºå€¼ï¼Œå–å¯¹æ•°(å¦‚æœå€¼å¾ˆå¤§)
        df = df.dropna()
        if df.max().max() > 50:
            df = np.log2(df + 1)
    except Exception as e:
        return None, f"Matrix parse error: {str(e)}"
    
    # å¯¹é½æ ·æœ¬
    valid_cols = [c for c in df.columns if c in conditions.index]
    df = df[valid_cols]
    
    case_cols = [c for c in df.columns if conditions.get(c) == "case"]
    ctrl_cols = [c for c in df.columns if conditions.get(c) == "control"]
    
    # 4. T-test
    results = []
    for gene, row in df.iterrows():
        case_vals = row[case_cols].values
        ctrl_vals = row[ctrl_cols].values
        if len(case_vals) < 2 or len(ctrl_vals) < 2: continue
        
        t, p = stats.ttest_ind(case_vals, ctrl_vals, equal_var=False)
        lfc = np.mean(case_vals) - np.mean(ctrl_vals)
        results.append({"gene": gene, "log2fc": lfc, "pval": p})
        
    res_df = pd.DataFrame(results).dropna()
    if res_df.empty: return None, "No valid DE results"
    
    # FDR
    res_df["padj"] = multipletests(res_df["pval"], method="fdr_bh")[1]
    res_df = res_df.sort_values("padj")
    
    # Map to Symbols (Simplified: assume index is roughly Symbol or needs mapping)
    # å®é™…åœºæ™¯å¯èƒ½éœ€è¦ ID mappingï¼Œè¿™é‡Œå‡è®¾ Matrix ä¸»è¦æ˜¯ Symbol æˆ–èƒ½è¢«å¤„ç†
    # ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬åªå–å‰ç¼€
    res_df["gene_symbol"] = res_df["gene"].apply(lambda x: str(x).split("//")[0].strip()) 
    
    return res_df, f"Success: Case={len(case_cols)}, Ctrl={len(ctrl_cols)}"

# --- Connectivity APIs ---

def run_l1000fwd(up_genes, dn_genes):
    url = "https://maayanlab.cloud/l1000fwd/sig_search"
    payload = {"up_genes": up_genes[:100], "down_genes": dn_genes[:100]}
    try:
        r = requests.post(url, json=payload)
        res_id = r.json().get("result_id")
        if not res_id: return pd.DataFrame()
        
        # Get Top results
        time.sleep(1)
        r2 = requests.get(f"https://maayanlab.cloud/l1000fwd/result/topn/{res_id}")
        data = r2.json()
        
        rows = []
        if "opposite" in data:
            for item in data["opposite"]:
                rows.append({
                    "drug": item.get("pert_id"), # è¿™é‡Œé€šå¸¸éœ€è¦äºŒæ¬¡æŸ¥è¯¢åå­—ï¼Œæš‚ç”¨ ID
                    "score": item.get("score"),
                    "source": "L1000FWD",
                    "direction": "opposite"
                })
        return pd.DataFrame(rows)
    except:
        return pd.DataFrame()

def run_enrichr(genes, library="LINCS_L1000_Chem_Pert_down"):
    base = "https://maayanlab.cloud/Enrichr"
    # Add List
    try:
        r = requests.post(f"{base}/addList", files={
            'list': (None, '\n'.join(genes)),
            'description': (None, 'demo')
        })
        user_list_id = r.json().get("userListId")
        
        # Enrich
        r2 = requests.get(f"{base}/enrich?userListId={user_list_id}&backgroundType={library}")
        data = r2.json()
        if library not in data: return pd.DataFrame()
        
        rows = []
        for item in data[library]:
            # item: [rank, term, pval, zscore, combined_score, ...]
            rows.append({
                "drug": item[1].split("_")[0], # ç®€å•æå–è¯å
                "score": item[4],
                "pval": item[2],
                "source": "Enrichr"
            })
        return pd.DataFrame(rows)
    except:
        return pd.DataFrame()

# ==========================================
# 2. Streamlit ç•Œé¢é€»è¾‘
# ==========================================

st.title("ğŸ’Š è‡ªåŠ¨åŒ–è¯ç‰©é‡å®šä½åˆ†æå¹³å° (Pipeline v5.2)")
st.markdown("åŸºäº GEO è½¬å½•ç»„æ•°æ® -> å·®å¼‚è¡¨è¾¾ -> L1000FWD/Enrichr -> è¯ç‰©æ¨è")

# --- Sidebar: è®¾ç½® ---
with st.sidebar:
    st.header("âš™ï¸ å‚æ•°è®¾ç½®")
    taxon_filter = st.selectbox("ç‰©ç§è¿‡æ»¤", ["Homo sapiens", "Mus musculus", "All"], index=0)
    st.divider()
    
    st.subheader("åˆ†ç»„æ¨æ–­å…³é”®è¯")
    default_case = "mutation, mutant, variant, patient, knockout, knockdown, disease, clcn"
    default_ctrl = "control, wt, wild type, healthy, normal, vehicle"
    
    case_input = st.text_area("å®éªŒç»„ (Case) å…³é”®è¯", default_case)
    ctrl_input = st.text_area("å¯¹ç…§ç»„ (Control) å…³é”®è¯", default_ctrl)
    
    case_terms = [x.strip().lower() for x in case_input.split(",")]
    ctrl_terms = [x.strip().lower() for x in ctrl_input.split(",")]
    
    st.divider()
    top_n_genes = st.number_input("Signature åŸºå› æ•°é‡", 50, 500, 150)

# --- Tab 1: æœç´¢ä¸é€‰æ‹© ---
tab1, tab2, tab3 = st.tabs(["1. æœç´¢ GEO æ•°æ®", "2. è¿è¡Œåˆ†ææµç¨‹", "3. è¯ç‰©æ’åºç»“æœ"])

with tab1:
    st.subheader("ğŸ” æœç´¢ GEO æ•°æ®é›†")
    col1, col2 = st.columns([3, 1])
    with col1:
        query_text = st.text_input("è¾“å…¥æŸ¥è¯¢ (ä¾‹å¦‚: CLCN2 mutation RNA-seq)", 
                                   value='(CLCN2 OR "chloride channel 2") AND (mutation OR knockout) AND "RNA-seq"')
    with col2:
        search_btn = st.button("å¼€å§‹æœç´¢", use_container_width=True)
    
    if search_btn and query_text:
        with st.spinner("æ­£åœ¨æœç´¢ NCBI GEO..."):
            df_hits = geo_search(query_text, retmax=50)
            if not df_hits.empty:
                if taxon_filter != "All":
                    df_hits = df_hits[df_hits["Taxon"] == taxon_filter]
                st.session_state["geo_hits"] = df_hits
            else:
                st.warning("æœªæ‰¾åˆ°ç›¸å…³æ•°æ®é›†ï¼Œè¯·å°è¯•æ”¾å®½æœç´¢æ¡ä»¶ã€‚")
    
    if not st.session_state["geo_hits"].empty:
        st.write(f"æ‰¾åˆ° {len(st.session_state['geo_hits'])} ä¸ªæ•°æ®é›†:")
        
        # ä½¿ç”¨ DataEditor è®©ç”¨æˆ·å‹¾é€‰
        hits_display = st.session_state["geo_hits"].copy()
        hits_display["Select"] = False
        edited_df = st.data_editor(hits_display, 
                                   column_config={"Select": st.column_config.CheckboxColumn(required=True)},
                                   disabled=["Accession", "Title", "Summary"],
                                   use_container_width=True)
        
        selected_gses = edited_df[edited_df["Select"]]["Accession"].tolist()
        st.session_state["selected_gses"] = selected_gses
        st.info(f"å·²é€‰æ‹© {len(selected_gses)} ä¸ª GSE è¿›è¡Œåˆ†æ: {', '.join(selected_gses)}")
    else:
        st.write("æš‚æ— æœç´¢ç»“æœã€‚")

# --- Tab 2: è¿è¡Œæµç¨‹ ---
with tab2:
    st.subheader("âš¡ æ‰¹å¤„ç†åˆ†æ")
    
    if "selected_gses" not in st.session_state or not st.session_state["selected_gses"]:
        st.warning("è¯·å…ˆåœ¨ç¬¬ 1 æ­¥é€‰æ‹©æ•°æ®é›†ã€‚")
    else:
        if st.button("ğŸš€ å¯åŠ¨åˆ†æ Pipeline", type="primary"):
            results_bucket = []
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            total_gse = len(st.session_state["selected_gses"])
            
            for i, gse in enumerate(st.session_state["selected_gses"]):
                status_text.text(f"æ­£åœ¨å¤„ç† {gse} ({i+1}/{total_gse})... ä¸‹è½½ & å·®å¼‚åˆ†æ")
                
                # 1. ä¸‹è½½ & å·®å¼‚åˆ†æ
                df_de, msg = run_ttest_pipeline(gse, case_terms, ctrl_terms)
                
                if df_de is None:
                    st.error(f"âŒ {gse}: å¤±è´¥ - {msg}")
                    continue
                
                st.success(f"âœ… {gse}: å·®å¼‚åˆ†æå®Œæˆ ({msg})")
                
                # 2. æå– Signature
                up_genes = df_de.sort_values("log2fc", ascending=False).head(top_n_genes)["gene_symbol"].tolist()
                dn_genes = df_de.sort_values("log2fc", ascending=True).head(top_n_genes)["gene_symbol"].tolist()
                
                # æ¸…æ´—åŸºå› å
                up_genes = clean_gene_list(up_genes)
                dn_genes = clean_gene_list(dn_genes)
                
                # 3. è¯ç‰©è¿æ¥æ€§é¢„æµ‹ (API Calls)
                status_text.text(f"æ­£åœ¨å¤„ç† {gse}... æŸ¥è¯¢ L1000FWD & Enrichr")
                
                # L1000FWD (å¯»æ‰¾åè½¬ä¿¡å· - Opposite)
                df_l1000 = run_l1000fwd(up_genes, dn_genes)
                
                # Enrichr (Input UP genes vs Drug DOWN lib = Reversal)
                df_enrichr = run_enrichr(up_genes, library="LINCS_L1000_Chem_Pert_down")
                
                # ç®€å•çš„ç»“æœåˆå¹¶
                combined_drugs = pd.concat([
                    df_l1000[["drug", "score", "source"]],
                    df_enrichr[["drug", "score", "source"]]
                ])
                
                if not combined_drugs.empty:
                    combined_drugs["gse"] = gse
                    results_bucket.append(combined_drugs)
                    with st.expander(f"{gse} åˆæ­¥å€™é€‰è¯ç‰© (Top 5)"):
                        st.dataframe(combined_drugs.head(5))
                
                progress_bar.progress((i + 1) / total_gse)
            
            status_text.text("åˆ†æå®Œæˆï¼æ­£åœ¨æ±‡æ€»...")
            
            if results_bucket:
                final_df = pd.concat(results_bucket)
                st.session_state["final_drug_rank"] = final_df
                st.balloons()
            else:
                st.warning("æ‰€æœ‰æ•°æ®é›†å‡æœªèƒ½äº§ç”Ÿæœ‰æ•ˆè¯ç‰©ç»“æœã€‚")

# --- Tab 3: ç»“æœå±•ç¤º ---
with tab3:
    st.subheader("ğŸ’Š è¯ç‰©æ’åºç»“æœ")
    
    res = st.session_state.get("final_drug_rank", pd.DataFrame())
    
    if not res.empty:
        # èšåˆè¯„åˆ†é€»è¾‘
        # 1. è®¡æ•°: å¤šå°‘ä¸ª GSE æ”¯æŒ
        # 2. å¹³å‡åˆ†: (æ³¨æ„ L1000å’ŒEnrichråˆ†æ•°å°ºåº¦ä¸åŒï¼Œè¿™é‡Œä»…ä½œæ¼”ç¤ºï¼Œå®é™…éœ€å½’ä¸€åŒ–)
        
        # ç®€å•çš„æ¸…æ´—è¯å
        res["drug_clean"] = res["drug"].str.lower().str.split("-").str[0]
        
        agg_df = res.groupby("drug_clean").agg(
            Count=('gse', 'nunique'),
            Sources=('source', lambda x: set(x)),
            GSEs=('gse', lambda x: ", ".join(set(x)))
        ).reset_index()
        
        agg_df = agg_df.sort_values("Count", ascending=False)
        
        col_res1, col_res2 = st.columns([2, 1])
        
        with col_res1:
            st.dataframe(agg_df, use_container_width=True, height=600)
            
        with col_res2:
            st.markdown("### ğŸ“Š ç»Ÿè®¡æ¦‚è§ˆ")
            st.metric("å‘ç°è¯ç‰©æ€»æ•°", len(agg_df))
            st.metric("é«˜é¢‘è¯ç‰© (>1 GSE)", len(agg_df[agg_df["Count"] > 1]))
            
            # ä¸‹è½½æŒ‰é’®
            csv = agg_df.to_csv(index=False).encode('utf-8')
            st.download_button(
                "ğŸ“¥ ä¸‹è½½å®Œæ•´è¯ç‰©åˆ—è¡¨ (CSV)",
                csv,
                "drug_repurposing_results.csv",
                "text/csv",
                key='download-csv'
            )
            
            st.markdown("---")
            st.markdown("**ä¸‹ä¸€æ­¥å»ºè®®:**")
            st.markdown("1. ä¸‹è½½ CSV æ–‡ä»¶")
            st.markdown("2. å°†è¯ç‰©åˆ—è¡¨å¯¼å…¥ PubChem æ‰¹é‡æŸ¥è¯¢ç»“æ„")
            st.markdown("3. è¿›è¡Œåˆ†å­å¯¹æ¥ (Docking) éªŒè¯")
            
    else:
        st.info("è¯·å…ˆåœ¨ Tab 2 è¿è¡Œåˆ†ææµç¨‹ã€‚")

# æ¸…ç†ä¸´æ—¶æ–‡ä»¶ (å¯é€‰)
# shutil.rmtree(WORK_DIR, ignore_errors=True)
